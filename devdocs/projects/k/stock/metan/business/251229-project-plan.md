# Project Plan: VN30F1M Intraday Trading AI Model

> **Status:** ğŸ“‹ PLANNING  
> **Last Updated:** 2026-01-03

## 1. Requirement

XÃ¢y dá»±ng má»™t application cÃ³ cÃ¡c chá»©c nÄƒng chÃ­nh:

1. âœ… **DONE** - Dá»±a vÃ o dá»¯ liá»‡u Ä‘Ã£ thu tháº­p Ä‘á»ƒ build ra cÃ¡c features â†’ `VN30FeaturePipeline`
2. DÃ¹ng AI sá»­ dá»¥ng features nÃ y Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ trong phiÃªn cá»§a VN30 (há»£p Ä‘á»“ng phÃ¡i sinh VN30F1M). LÆ°u Ã½ lÃ  chá»‰ náº¯m giá»¯ trong phiÃªn, báº¯t buá»™c sáº½ bÃ¡n khi Ä‘áº·t target profit, cháº¡m stop loss hoáº·c cuá»‘i phiÃªn.
   
Cá»¥ thá»ƒ AI model cáº§n sáº½ Ä‘Ã¡nh giÃ¡, tá»©c lÃ  sáº½ má»Ÿ lá»‡nh vÃ  Ä‘Ã³ng lá»‡nh táº¡i cÃ¡c thá»i Ä‘iá»ƒm phÃ¹ há»£p trong ngÃ y, Ä‘Æ°a ra hÃ nh Ä‘á»™ng vá»›i 3 trÆ°á»ng há»£p:

- VÃ o vá»‹ tháº¿ vÃ  dá»± Ä‘oÃ¡n GiÃ¡ tÄƒng X%
- VÃ o vá»‹ tháº¿ vÃ  dá»± Ä‘oÃ¡n GiÃ¡ giáº£m X%
- KhÃ´ng vÃ o vá»‹ tháº¿

---

## 2. AI Models cho Trading Prediction - Giáº£i thÃ­ch chi tiáº¿t

### 2.1 Tá»•ng quan cÃ¡c loáº¡i AI Models

Trong thá»±c táº¿, vá»›i bÃ i toÃ¡n dá»± Ä‘oÃ¡n giÃ¡ phÃ¡i sinh nhÆ° cá»§a báº¡n, cÃ³ **4 nhÃ³m chÃ­nh** cá»§a AI models:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI MODELS CHO TRADING PREDICTION                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ 1. TREE-BASED   â”‚  â”‚ 2. DEEP LEARNINGâ”‚  â”‚ 3. REINFORCEMENTâ”‚          â”‚
â”‚  â”‚    MODELS       â”‚  â”‚    (Neural Net) â”‚  â”‚    LEARNING     â”‚          â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚                 â”‚          â”‚
â”‚  â”‚ â€¢ Random Forest â”‚  â”‚ â€¢ LSTM          â”‚  â”‚ â€¢ DQN           â”‚          â”‚
â”‚  â”‚ â€¢ XGBoost       â”‚  â”‚ â€¢ GRU           â”‚  â”‚ â€¢ PPO           â”‚          â”‚
â”‚  â”‚ â€¢ LightGBM â­   â”‚  â”‚ â€¢ Transformer   â”‚  â”‚ â€¢ A2C           â”‚          â”‚
â”‚  â”‚ â€¢ CatBoost      â”‚  â”‚ â€¢ TCN           â”‚  â”‚                 â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ 4. ENSEMBLE & HYBRID                                        â”‚        â”‚
â”‚  â”‚                                                             â”‚        â”‚
â”‚  â”‚ Káº¿t há»£p nhiá»u models Ä‘á»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c                   â”‚        â”‚
â”‚  â”‚ â€¢ Stacking: LightGBM + LSTM                                 â”‚        â”‚
â”‚  â”‚ â€¢ Voting: Nhiá»u models vote cho quyáº¿t Ä‘á»‹nh                  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2.2 NhÃ³m 1: Tree-Based Models (Gradient Boosting)

#### ğŸŒ² CÃ¡ch hoáº¡t Ä‘á»™ng (Giáº£i thÃ­ch Ä‘Æ¡n giáº£n)

HÃ£y tÆ°á»Ÿng tÆ°á»£ng báº¡n há»i 100 chuyÃªn gia khÃ¡c nhau: "GiÃ¡ sáº½ tÄƒng hay giáº£m?"

- Má»—i chuyÃªn gia lÃ  má»™t "cÃ¢y quyáº¿t Ä‘á»‹nh" (decision tree)
- Má»—i cÃ¢y nhÃ¬n vÃ o cÃ¡c features (shark buy value, volume...) vÃ  Ä‘Æ°a ra dá»± Ä‘oÃ¡n
- Káº¿t quáº£ cuá»‘i cÃ¹ng = trung bÃ¬nh cá»§a táº¥t cáº£ cÃ¡c cÃ¢y

**Gradient Boosting** cáº£i tiáº¿n thÃªm:
- CÃ¢y sau há»c tá»« sai láº§m cá»§a cÃ¢y trÆ°á»›c
- Táº­p trung vÃ o nhá»¯ng trÆ°á»ng há»£p khÃ³ dá»± Ä‘oÃ¡n

#### ğŸ“Š So sÃ¡nh cÃ¡c Tree-Based Models

| Model | Tá»‘c Ä‘á»™ | Äá»™ chÃ­nh xÃ¡c | Äáº·c Ä‘iá»ƒm ná»•i báº­t |
|-------|--------|--------------|------------------|
| **Random Forest** | Nhanh | Tá»‘t | ÄÆ¡n giáº£n, Ã­t overfit |
| **XGBoost** | Trung bÃ¬nh | Ráº¥t tá»‘t | Regularization máº¡nh |
| **LightGBM** â­ | Ráº¥t nhanh | Ráº¥t tá»‘t | Tá»‘t nháº¥t cho tabular data |
| **CatBoost** | Cháº­m | Ráº¥t tá»‘t | Tá»‘t vá»›i categorical features |

#### âœ… Æ¯u Ä‘iá»ƒm

```
âœ“ Cá»±c ká»³ phÃ¹ há»£p vá»›i tabular/structured data (nhÆ° features cá»§a báº¡n)
âœ“ Tá»‘c Ä‘á»™ train vÃ  inference nhanh
âœ“ KhÃ´ng cáº§n nhiá»u data preprocessing (normalization, etc.)
âœ“ Feature importance rÃµ rÃ ng â†’ biáº¿t feature nÃ o quan trá»ng
âœ“ Ãt bá»‹ overfit náº¿u tune Ä‘Ãºng cÃ¡ch
âœ“ Dá»… debug vÃ  interpret káº¿t quáº£
```

#### âŒ NhÆ°á»£c Ä‘iá»ƒm

```
âœ— KhÃ´ng capture Ä‘Æ°á»£c temporal patterns tá»‘t (chuá»—i thá»i gian)
âœ— Má»—i sample Ä‘Æ°á»£c xá»­ lÃ½ Ä‘á»™c láº­p (khÃ´ng nhá»› context trÆ°á»›c Ä‘Ã³)
âœ— Cáº§n feature engineering thá»§ cÃ´ng (lag features, rolling features)
```

#### ğŸ¯ Khi nÃ o nÃªn dÃ¹ng?

- Báº¡n cÃ³ **tabular features** (nhÆ° whale footprint, OHLCV)
- Cáº§n **interpretability** (giáº£i thÃ­ch Ä‘Æ°á»£c táº¡i sao model quyáº¿t Ä‘á»‹nh)
- Muá»‘n **iterate nhanh** (train nhanh, thá»­ nhiá»u experiments)

---

### 2.3 NhÃ³m 2: Deep Learning (Neural Networks)

#### ğŸ§  CÃ¡ch hoáº¡t Ä‘á»™ng (Giáº£i thÃ­ch Ä‘Æ¡n giáº£n)

Neural network mÃ´ phá»ng cÃ¡ch nÃ£o ngÆ°á»i xá»­ lÃ½ thÃ´ng tin:
- **Input layer**: Nháº­n features (giÃ¡, volume, shark values...)
- **Hidden layers**: Xá»­ lÃ½ vÃ  tÃ¬m patterns phá»©c táº¡p
- **Output layer**: ÄÆ°a ra dá»± Ä‘oÃ¡n

#### ğŸ“Š CÃ¡c loáº¡i Deep Learning cho Time Series

##### a) LSTM (Long Short-Term Memory)

```
Candle 1 â†’ Candle 2 â†’ Candle 3 â†’ Candle 4 â†’ Candle 5 â†’ Prediction
    â†“         â†“         â†“         â†“         â†“
   [=========== MEMORY (nhá»› thÃ´ng tin quan trá»ng) ===========]
```

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
- LSTM cÃ³ "bá»™ nhá»›" Ä‘á»ƒ nhá»› thÃ´ng tin tá»« quÃ¡ khá»©
- 3 cá»•ng (gates) quyáº¿t Ä‘á»‹nh: nhá»› gÃ¬, quÃªn gÃ¬, output gÃ¬
- Tá»‘t cho viá»‡c tÃ¬m patterns trong chuá»—i dÃ i

**Æ¯u Ä‘iá»ƒm:**
- Capture Ä‘Æ°á»£c temporal dependencies (pattern theo thá»i gian)
- Tá»± Ä‘á»™ng há»c features tá»« raw data
- Tá»‘t khi cÃ³ nhiá»u data (>10,000 samples)

**NhÆ°á»£c Ä‘iá»ƒm:**
- Cháº­m hÆ¡n tree-based models 10-100x
- Cáº§n nhiá»u data vÃ  computing power
- Black box - khÃ³ giáº£i thÃ­ch
- Dá»… overfit náº¿u data Ã­t

##### b) Transformer (Self-Attention)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "Má»—i candle nhÃ¬n vÃ o Táº¤T Cáº¢ cÃ¡c candles khÃ¡c Ä‘á»ƒ        â”‚
â”‚  quyáº¿t Ä‘á»‹nh cÃ¡i nÃ o quan trá»ng cho prediction"          â”‚
â”‚                                                         â”‚
â”‚ Candle 1  Candle 2  Candle 3  Candle 4  Candle 5       â”‚
â”‚     â†‘         â†‘         â†‘         â†‘         â†‘          â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚              ATTENTION: CÃ¡i nÃ o quan trá»ng?             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
- KhÃ´ng xá»­ lÃ½ tuáº§n tá»± nhÆ° LSTM
- Má»—i time step "attend" (chÃº Ã½) Ä‘áº¿n táº¥t cáº£ time steps khÃ¡c
- TÃ¬m ra relationships báº¥t ká»³ Ä‘Ã¢u trong sequence

**Æ¯u Ä‘iá»ƒm:**
- Capture long-range dependencies tá»‘t hÆ¡n LSTM
- Song song hÃ³a Ä‘Æ°á»£c (train nhanh hÆ¡n LSTM)
- State-of-the-art cho nhiá»u bÃ i toÃ¡n NLP/time series

**NhÆ°á»£c Ä‘iá»ƒm:**
- Cáº§n Ráº¤T NHIá»€U data (100,000+ samples lÃ½ tÆ°á»Ÿng)
- Complex architecture, khÃ³ tune
- Overfitting dá»… xáº£y ra vá»›i data Ã­t

##### c) TCN (Temporal Convolutional Network)

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
- DÃ¹ng convolution thay vÃ¬ recurrence
- Dilated convolutions Ä‘á»ƒ capture long-range patterns
- Song song hÃ³a tá»‘t hÆ¡n LSTM

**Æ¯u Ä‘iá»ƒm:**
- Nhanh hÆ¡n LSTM
- KhÃ´ng bá»‹ vanishing gradient
- Tá»‘t cho fixed-length sequences

---

### 2.4 NhÃ³m 3: Reinforcement Learning (RL)

#### ğŸ® CÃ¡ch hoáº¡t Ä‘á»™ng (Giáº£i thÃ­ch Ä‘Æ¡n giáº£n)

Thay vÃ¬ dá»± Ä‘oÃ¡n giÃ¡, RL há»c trá»±c tiáº¿p **cÃ¡ch trading**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  REINFORCEMENT LEARNING                  â”‚
â”‚                                                         â”‚
â”‚   State (Features)  â†’  Agent  â†’  Action (LONG/SHORT)   â”‚
â”‚         â†‘                              â†“                â”‚
â”‚         â””â”€â”€â”€â”€ Reward (Profit/Loss) â†â”€â”€â”€â”˜                â”‚
â”‚                                                         â”‚
â”‚   Agent há»c tá»« trial-and-error:                         â”‚
â”‚   â€¢ Action tá»‘t â†’ Reward dÆ°Æ¡ng â†’ LÃ m nhiá»u hÆ¡n          â”‚
â”‚   â€¢ Action xáº¥u â†’ Reward Ã¢m â†’ TrÃ¡nh Ä‘i                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ğŸ“Š CÃ¡c thuáº­t toÃ¡n RL phá»• biáº¿n

| Algorithm | Äáº·c Ä‘iá»ƒm | PhÃ¹ há»£p cho |
|-----------|----------|-------------|
| **DQN** | Q-learning vá»›i neural network | Discrete actions (Buy/Sell/Hold) |
| **PPO** | Policy gradient, stable training | Continuous actions |
| **A2C/A3C** | Actor-Critic, parallel training | Large-scale training |

#### âœ… Æ¯u Ä‘iá»ƒm

```
âœ“ Há»c trá»±c tiáº¿p strategy, khÃ´ng chá»‰ prediction
âœ“ Tá»± Ä‘á»™ng tÃ­nh Ä‘áº¿n transaction costs, risk
âœ“ CÃ³ thá»ƒ optimize cho metrics cuá»‘i cÃ¹ng (Sharpe, PnL)
âœ“ KhÃ´ng cáº§n labeled data (self-supervised)
```

#### âŒ NhÆ°á»£c Ä‘iá»ƒm

```
âœ— Cá»±c ká»³ khÃ³ train - unstable, sensitive to hyperparameters
âœ— Cáº§n Ráº¤T NHIá»€U data vÃ  computing power
âœ— Dá»… overfit vÃ o historical patterns
âœ— Black box - khÃ´ng biáº¿t táº¡i sao agent lÃ m gÃ¬
âœ— KhÃ´ng phÃ¹ há»£p cho pilot/MVP
```

---

### 2.5 NhÃ³m 4: Ensemble & Hybrid

#### ğŸ”— CÃ¡ch hoáº¡t Ä‘á»™ng

Káº¿t há»£p nhiá»u models Ä‘á»ƒ táº­n dá»¥ng Æ°u Ä‘iá»ƒm cá»§a tá»«ng loáº¡i:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ENSEMBLE METHODS                      â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ LightGBM â”‚    â”‚   LSTM   â”‚    â”‚ XGBoost  â”‚          â”‚
â”‚  â”‚ (Tabular)â”‚    â”‚ (Temporal)â”‚   â”‚ (Robust) â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚
â”‚       â†“               â†“               â†“                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚            STACKING/VOTING                  â”‚        â”‚
â”‚  â”‚   Combine predictions â†’ Final Decision      â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### CÃ¡c phÆ°Æ¡ng phÃ¡p Ensemble

| Method | CÃ¡ch káº¿t há»£p |
|--------|-------------|
| **Voting** | Má»—i model vote, majority wins |
| **Averaging** | Trung bÃ¬nh predictions cá»§a cÃ¡c models |
| **Stacking** | Meta-model há»c cÃ¡ch combine cÃ¡c base models |
| **Blending** | Weighted average vá»›i weights há»c Ä‘Æ°á»£c |

---

### 2.6 So sÃ¡nh tá»•ng há»£p táº¥t cáº£ Models

| Criteria | Tree-Based | LSTM/GRU | Transformer | RL | Ensemble |
|----------|------------|----------|-------------|-----|----------|
| **Min Data** | 30 ngÃ y | 90 ngÃ y | 180+ ngÃ y | 360+ ngÃ y | 60 ngÃ y |
| **Train Speed** | âš¡ Nhanh | ğŸ¢ Cháº­m | ğŸ¢ Cháº­m | ğŸŒ Ráº¥t cháº­m | ğŸ¢ Cháº­m |
| **Inference** | âš¡ <1ms | ğŸ¢ 10-50ms | ğŸ¢ 50-100ms | âš¡ <10ms | ğŸ¢ Tá»•ng cÃ¡c models |
| **Accuracy*** | â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­ | â­â­â­â­ |
| **Interpretable** | âœ… CÃ³ | âŒ KhÃ´ng | âŒ KhÃ´ng | âŒ KhÃ´ng | ğŸ”¶ Pháº§n nÃ o |
| **Overfit Risk** | Tháº¥p | Cao | Ráº¥t cao | Ráº¥t cao | Trung bÃ¬nh |
| **Complexity** | Tháº¥p | Cao | Ráº¥t cao | Cá»±c cao | Cao |

*\*Accuracy phá»¥ thuá»™c nhiá»u vÃ o data vÃ  feature engineering*

---

## 3. Considerations & Answers (Updated)

### Data Availability

```
âœ… Training data: >180 ngÃ y (~10,800 samples)
âœ… Backtest data: 90 ngÃ y (~5,400 samples)
âœ… Total: ~270 ngÃ y (~16,200 samples)
```

Vá»›i lÆ°á»£ng data nÃ y, báº¡n cÃ³ thá»ƒ thá»­ nghiá»‡m háº§u háº¿t cÃ¡c models (trá»« pure Transformer vÃ  RL).

---

### Q1: NÃªn lá»±a chá»n model nÃ o Ä‘á»ƒ cÃ³ thá»ƒ Ä‘Æ°a ra Ä‘Æ°á»£c cáº£ 3 hÃ nh Ä‘á»™ng trÃªn?

### ğŸ¯ Recommendation: LightGBM + LSTM Ensemble

Vá»›i 180+ ngÃ y data, tÃ´i recommend **Ensemble approach**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               RECOMMENDED ARCHITECTURE                   â”‚
â”‚                                                         â”‚
â”‚   Features â”€â”€â”€â”¬â”€â”€â”€â†’ LightGBM â”€â”€â”€â”€â”                      â”‚
â”‚               â”‚                   â”‚                      â”‚
â”‚               â”‚                   â”œâ”€â”€â”€â†’ Meta-Model â”€â”€â†’ Prediction
â”‚               â”‚                   â”‚      (LightGBM)      â”‚
â”‚               â””â”€â”€â”€â†’ LSTM â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                         â”‚
â”‚   LightGBM: Capture tabular feature patterns            â”‚
â”‚   LSTM: Capture temporal/sequential patterns            â”‚
â”‚   Meta-Model: Learn optimal combination                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### PhÃ¢n tÃ­ch chi tiáº¿t

| TiÃªu chÃ­ | LightGBM only | LSTM only | **Ensemble (Recommended)** |
|----------|---------------|-----------|---------------------------|
| Tabular features | â­â­â­ Xuáº¥t sáº¯c | â­ KÃ©m | â­â­â­ Xuáº¥t sáº¯c |
| Temporal patterns | â­ KÃ©m | â­â­â­ Xuáº¥t sáº¯c | â­â­â­ Xuáº¥t sáº¯c |
| Train speed | â­â­â­ Nhanh | â­ Cháº­m | â­â­ Trung bÃ¬nh |
| Accuracy potential | â­â­ Tá»‘t | â­â­ Tá»‘t | â­â­â­ Cao nháº¥t |
| Robustness | â­â­ Tá»‘t | â­ Dá»… overfit | â­â­â­ Ráº¥t tá»‘t |
| Implementation | â­â­â­ Dá»… | â­â­ Trung bÃ¬nh | â­â­ Trung bÃ¬nh |

### Tuy nhiÃªn, Ä‘á»ƒ tá»‘i Æ°u thá»i gian, Ä‘á» xuáº¥t pilot strategy:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PILOT STRATEGY                         â”‚
â”‚                                                         â”‚
â”‚  Phase 1 (Tuáº§n 1): LightGBM only                        â”‚
â”‚  â”œâ”€â”€ Quick baseline                                     â”‚
â”‚  â”œâ”€â”€ Feature importance analysis                        â”‚
â”‚  â””â”€â”€ Validate data pipeline                             â”‚
â”‚                                                         â”‚
â”‚  Phase 2 (Tuáº§n 2): Add LSTM                             â”‚
â”‚  â”œâ”€â”€ Train LSTM on sequence data                        â”‚
â”‚  â””â”€â”€ Compare with LightGBM                              â”‚
â”‚                                                         â”‚
â”‚  Phase 3 (Tuáº§n 3): Ensemble                             â”‚
â”‚  â”œâ”€â”€ Stacking: LightGBM + LSTM                          â”‚
â”‚  â””â”€â”€ Final evaluation                                   â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**CÃ¡ch chuyá»ƒn tá»« Prediction â†’ 3 Actions:**

```python
# Model predict: price_change_percent (liÃªn tá»¥c)
predicted_change = ensemble_model.predict(features)  # e.g., +0.5%, -0.3%

# Convert to actions vá»›i dynamic threshold
LONG_THRESHOLD = 0.3    # Tune trÃªn validation set
SHORT_THRESHOLD = -0.3  # Tune trÃªn validation set

if predicted_change > LONG_THRESHOLD:
    action = "LONG"      # VÃ o vá»‹ tháº¿ vÃ  dá»± Ä‘oÃ¡n GiÃ¡ tÄƒng
    confidence = (predicted_change - LONG_THRESHOLD) / LONG_THRESHOLD
elif predicted_change < SHORT_THRESHOLD:
    action = "SHORT"     # VÃ o vá»‹ tháº¿ vÃ  dá»± Ä‘oÃ¡n GiÃ¡ giáº£m
    confidence = abs(predicted_change - SHORT_THRESHOLD) / abs(SHORT_THRESHOLD)
else:
    action = "HOLD"      # KhÃ´ng vÃ o vá»‹ tháº¿
    confidence = 0
```

**Æ¯u Ä‘iá»ƒm cá»§a Ensemble approach:**
- Thresholds cÃ³ thá»ƒ tune sau khi train
- CÃ³ thá»ƒ scale position size dá»±a vÃ o confidence
- LightGBM capture whale footprint patterns tá»‘t
- LSTM capture momentum/trend patterns
- Meta-model há»c cÃ¡ch káº¿t há»£p tá»‘i Æ°u

---

### Q2: Sá»‘ lÆ°á»£ng ngÃ y cáº§n dÃ¹ng Ä‘á»ƒ train?

**Vá»›i data availability cá»§a báº¡n:**

```
Total: 270 ngÃ y (~16,200 candles)

Data Split Ä‘á» xuáº¥t:
â”œâ”€â”€ Train: 180 ngÃ y (67%) â”€â”€â”€â”€â”€â†’ ~10,800 samples
â”œâ”€â”€ Validation: 45 ngÃ y (17%) â”€â†’ ~2,700 samples
â””â”€â”€ Test/Backtest: 45 ngÃ y (17%) â†’ ~2,700 samples

Hoáº·c sá»­ dá»¥ng Walk-Forward:
â”œâ”€â”€ Initial Train: 120 ngÃ y
â”œâ”€â”€ Walk-Forward Window: 30 ngÃ y
â””â”€â”€ Test: 30 ngÃ y (rolling)
```

**LÆ°u Ã½ quan trá»ng:**
- Pháº£i split theo thá»i gian (khÃ´ng random shuffle) Ä‘á»ƒ trÃ¡nh data leakage
- Validation set dÃ¹ng Ä‘á»ƒ tune hyperparameters & thresholds
- Test set chá»‰ dÃ¹ng 1 láº§n cuá»‘i cÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ final model

---

## 3. Available Data Summary

**Data Source:** `stock_trading_feature_candles` table (symbol="VN30")

### 3.1 Price Features (OHLCV)
| Feature | Description | Unit |
|---------|-------------|------|
| `open`, `high`, `low`, `close` | VN30 Index OHLCV | Index points |
| `volume` | Total volume 30 stocks | Shares |
| `value` | Total traded value | Million VND |

### 3.2 Whale Footprint Features
| Feature | Description |
|---------|-------------|
| `vn30_shark450_buy_value` | GiÃ¡ trá»‹ mua cá»§a cÃ¡ máº­p (â‰¥450M) |
| `vn30_shark450_sell_value` | GiÃ¡ trá»‹ bÃ¡n cá»§a cÃ¡ máº­p (â‰¥450M) |
| `vn30_shark900_buy_value` | GiÃ¡ trá»‹ mua cá»§a cÃ¡ máº­p lá»›n (â‰¥900M) |
| `vn30_shark900_sell_value` | GiÃ¡ trá»‹ bÃ¡n cá»§a cÃ¡ máº­p lá»›n (â‰¥900M) |
| `vn30_shark450_buy_ratio_5d_pc` | Ratio vs 5-day baseline |
| `vn30_percent_shark450_buy_sell` | % mua trong tá»•ng flow cÃ¡ máº­p |
| `vn30_shark450_urgency_spread` | VWAP urgency indicator |

---

## 5. Implementation Plan (Updated for Ensemble)

### Phase 1: Data Preparation (2-3 ngÃ y)

**Má»¥c tiÃªu:** Chuáº©n bá»‹ dataset cho cáº£ LightGBM vÃ  LSTM

1. **Export VN30 features tá»« DB**
   ```python
   # Query tá»« stock_trading_feature_candles
   # Filter: symbol="VN30", interval=300
   # Range: 270 ngÃ y data
   ```

2. **Táº¡o Target Variable (Y)**
   ```python
   # Primary: Next candle return (for regression)
   y_regression = (next_close - current_close) / current_close * 100
   
   # Secondary: Direction classification (for validation)
   y_classification = 1 if y_regression > 0.3 else (-1 if y_regression < -0.3 else 0)
   ```

3. **Feature Engineering cho LightGBM**
   ```python
   # Lag features (point-in-time values from previous candles)
   for col in ['close', 'volume', 'vn30_shark450_buy_value', ...]:
       for lag in [1, 2, 3, 5, 10]:
           df[f'{col}_lag_{lag}'] = df[col].shift(lag)
   
   # Returns
   for lag in [1, 5, 10, 20]:
       df[f'return_{lag}'] = df['close'].pct_change(lag) * 100
   
   # Rolling statistics
   for window in [5, 10, 20]:
       df[f'close_ma_{window}'] = df['close'].rolling(window).mean()
       df[f'volume_std_{window}'] = df['volume'].rolling(window).std()
   
   # Time features
   df['hour'] = df['time'].dt.hour
   df['minute'] = df['time'].dt.minute
   df['candle_of_day'] = (df['time'] - df['time'].dt.normalize()).dt.seconds // 300
   ```

4. **Feature Engineering cho LSTM**
   ```python
   # LSTM cáº§n sequences, khÃ´ng cáº§n lag features
   # Normalize features to [0, 1] or standardize
   
   sequence_length = 20  # 20 candles lookback (~100 minutes)
   features_for_lstm = ['close', 'volume', 'vn30_shark450_buy_value', ...]
   
   # Create sequences: shape (samples, sequence_length, num_features)
   X_lstm = create_sequences(df[features_for_lstm], sequence_length)
   ```

5. **Data Split**
   ```
   Total: 270 ngÃ y (~16,200 candles)
   â”œâ”€â”€ Train: Day 1-180 (~10,800 samples)
   â”œâ”€â”€ Validation: Day 181-225 (~2,700 samples)
   â””â”€â”€ Test: Day 226-270 (~2,700 samples)
   ```

---

### Phase 2: LightGBM Baseline (3-4 ngÃ y)

**Má»¥c tiÃªu:** Train LightGBM vÃ  establish baseline performance

1. **Train LightGBM**
   ```python
   import lightgbm as lgb
   
   params = {
       "objective": "regression",
       "metric": "mae",
       "num_leaves": 63,
       "learning_rate": 0.03,
       "feature_fraction": 0.8,
       "bagging_fraction": 0.8,
       "bagging_freq": 5,
       "min_child_samples": 20,
       "lambda_l1": 0.1,
       "lambda_l2": 0.1,
   }
   
   model_lgb = lgb.train(
       params,
       train_data,
       valid_sets=[val_data],
       num_boost_round=1000,
       callbacks=[
           lgb.early_stopping(100),
           lgb.log_evaluation(50),
       ],
   )
   ```

2. **Hyperparameter Tuning vá»›i Optuna**
   ```python
   import optuna
   
   def objective(trial):
       params = {
           "num_leaves": trial.suggest_int("num_leaves", 20, 150),
           "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.1),
           "feature_fraction": trial.suggest_float("feature_fraction", 0.5, 1.0),
           ...
       }
       # Train and return validation MAE
       return validation_mae
   
   study = optuna.create_study(direction="minimize")
   study.optimize(objective, n_trials=100)
   ```

3. **Feature Importance Analysis**
   ```python
   importance = model_lgb.feature_importance(importance_type='gain')
   # Visualize top 20 features
   # Remove features with near-zero importance
   ```

4. **Baseline Metrics**
   - MAE, RMSE trÃªn validation set
   - Directional accuracy
   - Quick backtest Ä‘á»ƒ check viability

---

### Phase 3: LSTM Model (4-5 ngÃ y)

**Má»¥c tiÃªu:** Train LSTM Ä‘á»ƒ capture temporal patterns

1. **LSTM Architecture**
   ```python
   import torch
   import torch.nn as nn
   
   class LSTMPredictor(nn.Module):
       def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2):
           super().__init__()
           self.lstm = nn.LSTM(
               input_size=input_size,
               hidden_size=hidden_size,
               num_layers=num_layers,
               batch_first=True,
               dropout=dropout,
           )
           self.fc = nn.Sequential(
               nn.Linear(hidden_size, 32),
               nn.ReLU(),
               nn.Dropout(0.2),
               nn.Linear(32, 1),  # Predict price change %
           )
       
       def forward(self, x):
           lstm_out, _ = self.lstm(x)
           last_hidden = lstm_out[:, -1, :]  # Take last timestep
           return self.fc(last_hidden)
   ```

2. **Training Loop**
   ```python
   model = LSTMPredictor(input_size=len(features), hidden_size=128)
   optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
   scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)
   
   for epoch in range(100):
       train_loss = train_epoch(model, train_loader)
       val_loss = validate(model, val_loader)
       scheduler.step(val_loss)
       
       # Early stopping
       if early_stopper.should_stop(val_loss):
           break
   ```

3. **Hyperparameter Tuning**
   - Hidden size: [64, 128, 256]
   - Num layers: [1, 2, 3]
   - Sequence length: [10, 20, 30, 50]
   - Dropout: [0.1, 0.2, 0.3]

4. **Compare vá»›i LightGBM**
   - So sÃ¡nh MAE, directional accuracy
   - Analyze cases where LSTM > LightGBM vÃ  ngÆ°á»£c láº¡i

---

### Phase 4: Ensemble (3-4 ngÃ y)

**Má»¥c tiÃªu:** Combine LightGBM + LSTM Ä‘á»ƒ maximize accuracy

1. **Stacking Architecture**
   ```python
   # Level 1: Base models
   pred_lgb = model_lgb.predict(X_val_lgb)      # LightGBM predictions
   pred_lstm = model_lstm.predict(X_val_lstm)   # LSTM predictions
   
   # Level 2: Meta-model
   meta_features = np.column_stack([
       pred_lgb,
       pred_lstm,
       # Optionally: original features
   ])
   
   meta_model = lgb.LGBMRegressor()
   meta_model.fit(meta_features_train, y_train)
   
   # Final prediction
   final_pred = meta_model.predict(meta_features_test)
   ```

2. **Alternative: Weighted Average**
   ```python
   # Learn optimal weights on validation set
   def find_optimal_weights(pred_lgb, pred_lstm, y_true):
       best_weight = 0.5
       best_mae = float('inf')
       
       for w in np.arange(0.1, 0.9, 0.05):
           combined = w * pred_lgb + (1-w) * pred_lstm
           mae = mean_absolute_error(y_true, combined)
           if mae < best_mae:
               best_mae = mae
               best_weight = w
       
       return best_weight
   
   # Typical result: LightGBM 60-70%, LSTM 30-40%
   ```

3. **Threshold Optimization**
   ```python
   def optimize_thresholds(predictions, y_true, metric='sharpe'):
       best_thresholds = (0.3, -0.3)
       best_metric = -float('inf')
       
       for long_th in np.arange(0.1, 0.6, 0.05):
           for short_th in np.arange(-0.6, -0.1, 0.05):
               signals = get_signals(predictions, long_th, short_th)
               returns = calculate_returns(signals, y_true)
               
               if metric == 'sharpe':
                   score = calculate_sharpe(returns)
               elif metric == 'profit_factor':
                   score = calculate_profit_factor(returns)
               
               if score > best_metric:
                   best_metric = score
                   best_thresholds = (long_th, short_th)
       
       return best_thresholds
   ```

---

### Phase 5: Backtesting (3-4 ngÃ y)

**Má»¥c tiÃªu:** Validate strategy vá»›i realistic trading simulation

1. **Trading Simulator**
   ```python
   class TradingSimulator:
       def __init__(self, initial_capital=100_000_000):  # 100M VND
           self.capital = initial_capital
           self.position = 0  # -1, 0, 1
           self.entry_price = 0
           self.trades = []
           
           # Hyperparameters (tunable)
           self.take_profit = 0.5  # 0.5%
           self.stop_loss = 0.3    # 0.3%
           self.transaction_cost = 0.0003  # 0.03%
       
       def step(self, prediction, current_price, long_th, short_th):
           # Exit logic first
           if self.position != 0:
               pnl = (current_price - self.entry_price) / self.entry_price
               pnl *= self.position  # Adjust for short
               
               if pnl >= self.take_profit or pnl <= -self.stop_loss:
                   self._close_position(current_price)
           
           # Entry logic
           if self.position == 0:
               if prediction > long_th:
                   self._open_position(1, current_price)  # LONG
               elif prediction < short_th:
                   self._open_position(-1, current_price)  # SHORT
   ```

2. **Walk-Forward Validation**
   ```python
   # Rolling window training
   window_size = 120  # 120 days training
   step_size = 20     # Re-train every 20 days
   
   results = []
   for start in range(0, len(data) - window_size - 30, step_size):
       train_end = start + window_size
       test_end = train_end + 30
       
       # Train on window
       model = train_ensemble(data[start:train_end])
       
       # Test on next 30 days
       test_result = backtest(model, data[train_end:test_end])
       results.append(test_result)
   
   # Analyze stability across windows
   ```

3. **Metrics Dashboard**
   | Metric | Target | Description |
   |--------|--------|-------------|
   | Win Rate | > 52% | % trades profitable |
   | Profit Factor | > 1.3 | Gross profit / Gross loss |
   | Max Drawdown | < 8% | Largest peak-to-trough |
   | Sharpe Ratio | > 1.5 | Risk-adjusted return |
   | Avg Trades/Day | 5-15 | Not too few, not too many |
   | Avg Holding Time | 15-60 min | Intraday only |

---

### Phase 6: Production Pipeline (4-5 ngÃ y)

**Má»¥c tiÃªu:** Deploy model Ä‘á»ƒ cháº¡y realtime

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRODUCTION ARCHITECTURE                       â”‚
â”‚                                                                 â”‚
â”‚  Every 5 minutes:                                               â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ VN30Feature  â”‚â”€â”€â”€â†’â”‚ Feature      â”‚â”€â”€â”€â†’â”‚ Ensemble     â”‚      â”‚
â”‚  â”‚ Pipeline     â”‚    â”‚ Engineering  â”‚    â”‚ Model        â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                 â”‚               â”‚
â”‚                                                 â†“               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Order        â”‚â†â”€â”€â”€â”‚ Position     â”‚â†â”€â”€â”€â”‚ Signal       â”‚      â”‚
â”‚  â”‚ Executor     â”‚    â”‚ Manager      â”‚    â”‚ Generator    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚        â”‚                                                        â”‚
â”‚        â†“                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚  â”‚ VN30F1M      â”‚                                              â”‚
â”‚  â”‚ Exchange     â”‚                                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. Project Timeline (Updated)

| Phase | Task | Duration | Output |
|-------|------|----------|--------|
| **1** | Data Preparation | 2-3 ngÃ y | Clean dataset, feature engineering |
| **2** | LightGBM Baseline | 3-4 ngÃ y | Tuned LightGBM model |
| **3** | LSTM Model | 4-5 ngÃ y | Trained LSTM model |
| **4** | Ensemble | 3-4 ngÃ y | Stacked ensemble model |
| **5** | Backtesting | 3-4 ngÃ y | Performance report, walk-forward results |
| **6** | Production Pipeline | 4-5 ngÃ y | Realtime prediction system |
| | **Total** | **19-25 ngÃ y (~4-5 tuáº§n)** | |

---

## 7. Success Criteria

| Criteria | Minimum | Target | Measurement |
|----------|---------|--------|-------------|
| Directional Accuracy | > 52% | > 55% | % Ä‘Ãºng hÆ°á»›ng trÃªn test set |
| MAE | < 0.5% | < 0.35% | Mean Absolute Error |
| Win Rate | > 50% | > 55% | % trades profitable |
| Profit Factor | > 1.2 | > 1.5 | Gross profit / Gross loss |
| Sharpe Ratio | > 1.0 | > 1.5 | Annualized risk-adjusted return |
| Max Drawdown | < 12% | < 8% | Largest peak-to-trough |
| Walk-forward Consistency | > 60% | > 75% | % windows profitable |

---

## 8. Next Steps (Immediate)

1. [ ] Verify data trong DB: count sá»‘ ngÃ y, check missing dates
2. [ ] Setup project structure cho AI package (metan-ai hoáº·c trong stock package)
3. [ ] Implement DataLoader class Ä‘á»ƒ fetch vÃ  prepare data
4. [ ] Create Jupyter notebook cho EDA vÃ  feature exploration
5. [ ] Install dependencies: lightgbm, torch, optuna, pandas

---

## 9. Risk Mitigation

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Overfitting | Cao | Cao | Walk-forward validation, regularization |
| Data leakage | Trung bÃ¬nh | Cao | Strict time-based splits, careful feature engineering |
| Model degradation | Cao | Trung bÃ¬nh | Daily monitoring, periodic retraining |
| Market regime change | Cao | Cao | Ensemble diversification, regime detection |
| Slippage/execution | Trung bÃ¬nh | Trung bÃ¬nh | Conservative sizing, limit orders |

---

## 10. Future Enhancements (Post-Pilot)

1. **Additional Models**
   - TCN (Temporal Convolutional Network)
   - Attention-based LSTM
   - XGBoost as another base model

2. **Additional Features**
   - VN30F1M premium/discount vs VN30 spot
   - Bid-ask spread, order book imbalance
   - Sector rotation signals
   - Macro indicators (USD/VND, oil price)

3. **Advanced Risk Management**
   - Dynamic position sizing based on confidence
   - Correlation with VIX/market conditions
   - Kelly criterion position sizing
   - Regime-aware trading rules
